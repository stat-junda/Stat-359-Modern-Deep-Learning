{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    dataset_size = 20000\n",
    "    time_horizon = 10\n",
    "    features = 1\n",
    "    dataset_x = np.random.randint(low=0, high=10, size=(dataset_size, time_horizon, features))\n",
    "    dataset_y = dataset_x.sum(axis=1)  # sum the time dimension.\n",
    "    return dataset_x, dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() is True:\n",
    "    device = 'cuda'\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#device = 'cpu'\n",
    "\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a reference multi-head attention implementation. You should just use the one implemented in torch.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_dim // num_heads\n",
    "\n",
    "        self.query_projection = nn.Linear(input_dim, input_dim)\n",
    "        self.key_projection = nn.Linear(input_dim, input_dim)\n",
    "        self.value_projection = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        self.out_projection = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Project queries, keys, and values\n",
    "        q = self.query_projection(x)\n",
    "        k = self.key_projection(x)\n",
    "        v = self.value_projection(x)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(batch_size, self.num_heads, -1, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, self.num_heads, -1, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, self.num_heads, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.head_dim**0.5\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        x = torch.matmul(weights, v)\n",
    "\n",
    "        # Concatenate heads and project back to the original dimension\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.input_dim)\n",
    "        x = self.out_projection(x)\n",
    "\n",
    "        return x, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.layer_norms[0](x)\n",
    "        x, _ = self.multi_head_attention(x, x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.layer_norms[1](x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, dropout, output_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(hidden_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_projection = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        self.final_proojection = nn.Linear(input_dim, output_dim)\n",
    "        # this is added to reduce the time dimension down to 1 and is not a standard part of the model.\n",
    "        # self.output_time_dimension_reduction = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_projection(x)\n",
    "\n",
    "\n",
    "\n",
    "        # alternatively\n",
    "        #x = torch.sum(x, dim=1)\n",
    "        #x = self.final_proojection(x)\n",
    "\n",
    "        x = x[:, 0, :]\n",
    "        x = self.final_proojection(x) # strictly speaking this is not needed.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.model = Transformer(dropout=0.1, hidden_dim=12, input_dim=1, num_heads=4, num_layers=2, output_dim=1)\n",
    "        self.model.to(device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "\n",
    "    def train(self, x_mb, y_mb):\n",
    "        x_mb = torch.from_numpy(x_mb.astype(np.float32)).to(device=device)\n",
    "        y_hat = self.model(x_mb)\n",
    "        y_mb = torch.tensor([y_mb]).float().to(device)\n",
    "        loss = self.criterion(y_hat, y_mb)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip = False\n",
    "        if clip is True:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = x.astype(np.float32)\n",
    "        if len(x.shape) < 3:\n",
    "            x = np.expand_dims(x, 0)\n",
    "        x = torch.from_numpy(x)\n",
    "        y_hat = self.model(x)\n",
    "        y_hat = y_hat.detach().cpu().numpy()\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_x, dataset_y = create_dataset()\n",
    "    trainer = Trainer()\n",
    "    batch_size = 32\n",
    "\n",
    "    one_y_mb = trainer.predict(dataset_x[0:32, :, :])\n",
    "\n",
    "    for i in range(10000):\n",
    "        idxs = np.random.randint(len(dataset_x), size=batch_size)\n",
    "        x_mb = dataset_x[idxs]\n",
    "        y_mb = dataset_y[idxs]\n",
    "        loss = trainer.train(x_mb, y_mb)\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        if i % 1000 == 0:\n",
    "            print(F\"For Epoch {i} loss is {loss}\")\n",
    "            print(F\"Given vector {dataset_x[0, :, 0]} the solution is {dataset_y[0, 0]}\")\n",
    "            one_y = trainer.predict(dataset_x[0, :, :])[0][0]\n",
    "            print(F\"The predicted solution is: {one_y}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
