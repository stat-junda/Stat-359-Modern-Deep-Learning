{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning LLM\n",
        "\n",
        "We will learn how to finetune a small scale LLM: OPT-350m\n",
        "\n",
        "We will fine-tune OPT-350m to generate coherent stories, acknowledging that its limited capabilities may result in stories comparable to a first grader's level. However, this approach should still yield improved outcomes compared to using the model without fine-tuning.\n",
        "\n",
        "First, connect to a T4 GPU instance\n",
        "\n",
        "Then we need to install and load the necessary packages.\n"
      ],
      "metadata": {
        "id": "iSwoF_xmeUTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install accelerate bitsandbytes peft datasets transformers"
      ],
      "metadata": {
        "id": "KYLVBLZdeWZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`accelerate`, `bitsandbytes` are both used for reducing memory requirements to speed up the training process\n",
        "\n",
        "`peft` stands for parameter efficient fine tuning. This is where LoRA is housed.\n",
        "\n",
        "`datasets` allows you to load data sets from HuggingFace, and `transformers` is a wrapper for transformer based models on HF."
      ],
      "metadata": {
        "id": "Q_aVv0aygZB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\",\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        ")"
      ],
      "metadata": {
        "id": "-x8qV2neg4sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers are required for LLMs. Complete the `tokenizer` variable by using the `AutoTokenizer` class which inherits from Tokenizer. Make sure you use the appropriate tokenizer.\n",
        "\n",
        "(You should read up on how to use Tokenizers https://github.com/huggingface/tokenizers/blob/main/README.md)"
      ],
      "metadata": {
        "id": "-JMv4B3jhynq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = None # Placeholder"
      ],
      "metadata": {
        "id": "0wwnD7rEhDGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizers\n",
        "\n",
        "Tokenizers convert words into subwords and assigns them an ID. We will learn to play with tokenizers here.\n",
        "\n",
        "Using the loaded tokenizer, find the token ids for the string \"Northwestern Wildcats\".\n",
        "\n",
        "(Make sure you have the correct tokenizer, or the results for the rest of the assignment will not be correct)."
      ],
      "metadata": {
        "id": "a8RaF7R6tT3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = None # placeholder\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STnEQUpFtho8",
        "outputId": "e00e7c61-3a36-48b9-9323-17a792427e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An encoded message is shown below as a sequence of token IDs. Please decode the message with the tokenizer."
      ],
      "metadata": {
        "id": "yFRW8IvNuUF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [2, 11073, 16507, 589, 36, 487, 791, 43, 16, 10, 940, 557, 2737, 11, 9771, 6712, 6,\n",
        " 3882, 6, 315, 532, 4, 5441, 28477, 11, 504, 4708, 7, 1807, 5, 3575, 8535, 23463, 6,\n",
        " 24, 16, 5, 7763, 5966, 3215, 2737, 11, 3882, 4, 20, 2737, 34, 63, 1049, 2894, 552, 5,\n",
        " 20597, 9, 1777, 2293, 11, 5, 1568, 20887, 443, 4, 1437]\n",
        "\n",
        "decoded_string = None # placeholder\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYQcLr6FtzFL",
        "outputId": "f6def7e1-1b2d-466e-dd61-16b0694fef20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA\n",
        "\n",
        "The transformer model and its tokenizer has been defined. Now we need to attach a LoRA adapter if we hope to train the model at all.\n",
        "\n",
        "LoRA has some parameters for you to tune. Please fill out the appropriate `task_type`.\n",
        "\n",
        "Please also fill out `r` and `lora_alpha`. These are tunable hyperparameters and you can come back and edit these two as you see fit.\n",
        "\n",
        "Please read https://huggingface.co/docs/peft/main/en/developer_guides/lora for a guide on these parameters\n"
      ],
      "metadata": {
        "id": "zzzg2fyqjT6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=0, # placeholder\n",
        "    lora_alpha=0, # placeholder\n",
        "    target_modules= [\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout= 0.05,\n",
        "    bias=\"none\",\n",
        "    task_type= None # placeholder\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "jy5rdwPxjTL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LoRA model has been set. To see if it has actually reduced the number of trainable parameters, apply the following function on your lora model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QNx8LSHPk_IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Input: torch model\n",
        "\n",
        "    Return: None. Print message instead\n",
        "\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    Report the percentage of trainable parameters / all parameters\n",
        "    \"\"\"\n",
        "\n",
        "    # Hint: keep two counters initialized at 0\n",
        "    # iterate through all parameters and keep track of which\n",
        "    # parameters require gradients\n",
        "    # Report\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        pass # placeholder\n",
        "\n",
        "    print(\n",
        "        f\"trainable params: {None}\" # placeholder\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(lora_model)"
      ],
      "metadata": {
        "id": "f-qWoE26l0_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TinyStories\n",
        "\n",
        "The model has been set with the LoRA adapter. Now we are ready to collect our dataset. We will be using a subset of TinyStories which is a collection of ~2-5 sentence stories.\n"
      ],
      "metadata": {
        "id": "xdi52vc_mO-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"roneneldan/TinyStories\", split='train[0:5000]')\n",
        "data['text'][0]"
      ],
      "metadata": {
        "id": "XyU0yNfGmKwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data has been tokenized for you in the cell below."
      ],
      "metadata": {
        "id": "Sw7dHp-1nP34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(data):\n",
        "    return tokenizer(data['text'])\n",
        "tokenized_data = data.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "tokenized_data"
      ],
      "metadata": {
        "id": "Egoqfz1ynPVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset has 5000 rows, and it contains the columns `input_ids` and `attention_mask`.\n",
        "\n",
        "Please describe what the `input_id` and `attention_mask` are."
      ],
      "metadata": {
        "id": "R3y3HmRynoTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Your response here_"
      ],
      "metadata": {
        "id": "AehwcZx0n9Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to speed up training, we concatenate all 5000 rows of stories into one long block of text. Then we will chunk the block of text into chunks of size 128. Feel free to experiment with this number."
      ],
      "metadata": {
        "id": "iHBWtUyYn_Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples, block_size=128):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()} # input ids and attention masks, concat these lists\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]]) # get total length of input ids, should be equal to mask length\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size # delete remainder given block size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "processed_datasets = tokenized_data.map(group_texts,\n",
        "                                        batched=True,\n",
        "                                        batch_size=1000,\n",
        "                                        num_proc=4,)"
      ],
      "metadata": {
        "id": "iGF5TDfonnkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use your tokenizer to decode the input ids for chunk 1."
      ],
      "metadata": {
        "id": "LfqTThd3p105"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = processed_datasets[1][\"input_ids\"]\n",
        "text = None # placeholder\n",
        "print(text)"
      ],
      "metadata": {
        "id": "AvGGB-FspxZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we train, we look at the model output when we prompt it with a story with \"Alice and Bob\". Run the cell below to see what the default OPT-350m will give when prompted with Alice and Bob.\n",
        "\n",
        "Decode the model generated tokens and print the story."
      ],
      "metadata": {
        "id": "pcazsbAYqDl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer('Alice and Bob', return_tensors='pt').to('cuda')\n",
        "greedy_output = model.generate(**model_inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)[0]\n",
        "story = None # placeholder\n",
        "print(story)"
      ],
      "metadata": {
        "id": "Xflrr3BopxOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "\n",
        "Now we begin our training loop. We will use the HuggingFace trainer API since it has built-in efficiencies. Please fill in the `per_device_train_batch_size`, `gradient_accumulation_steps`, `learning_rate`, and `num_train_epochs`.\n",
        "\n",
        "- `per_device_train_batch_size`: Assuming one device (one GPU), this determines the batch size you use.\n",
        "- `gradient_accumulation_steps`: This determines the number of forward passes to take, and accumulate losses, before taking a backward pass to update model parameters.\n",
        "\n",
        "These two parameters effectively determine how much data goes into estimating your gradient. More data leads to more accurate gradient estimations, but becomes memory intensive. Modify these two parameters in tandem for efficiency.\n",
        "\n",
        "Make sure you train for enough epochs. Even with the built-in efficiencies, training takes a while. Be sure to budget your time for this portion."
      ],
      "metadata": {
        "id": "xL25CnI7qfqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=lora_model,\n",
        "    train_dataset=processed_datasets,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=0, #placeholder,\n",
        "        gradient_accumulation_steps=0, #placeholder,\n",
        "        learning_rate=0, #placeholder,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "        num_train_epochs=0 # placeholder\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=None) # Fill out the None to be either True or False. Which one is it?\n",
        ")\n",
        "lora_model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-8KlkD3Jqet9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model has trained, write the following code to visualize the output for the story prompt \"Alice and Bob\""
      ],
      "metadata": {
        "id": "GynomapzsOMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer('Alice and Bob', return_tensors='pt').to('cuda')\n",
        "output = # placeholder\n",
        "tuned_story = print(output)"
      ],
      "metadata": {
        "id": "td86tCBjsXzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modify the Generator\n",
        "\n",
        "How can we make this better?\n",
        "\n",
        "model.generate takes the input, passes it through the LLM, and selects tokens to be decoded in a probabilitic manner. It can be controlled by the following:\n",
        "\n",
        "- `beam search = k`: This means that instead of looking at probabilities of the next single token, the model will consider probabilities over the next `k` tokens.\n",
        "\n",
        "- `do_sample`: Tells the model whether to sample for the next tokens, or pick the next best token.\n",
        "\n",
        "- `top-k = k`: Over the probability distribution of the next possible tokens, we filter out only the tokens with the top `k` highest probabilities. The probability is redistributed over these `k` tokens and we can sample from this.\n",
        "\n",
        "- `top-p = p`: Over the probability distribution of the next possible tokens, we keep the set of tokens with highest probabilities, such that they all sum to `p`. Then we sample over these tokens.\n",
        "\n",
        "- `temperature = T`: It makes the distribution over the the next tokens sharper. That is, higher temperatures make the distribution more uniform, while lower temperatures increase the differences in probabilities between tokens. This is essentially a way pronounce probability differences in a distribution.\n",
        "\n",
        "- `no_repeat_ngram_size=n`: Stops the model from repeating any sequence of n tokens.\n",
        "\n",
        "Think about how each of these parameters affect how we sample the next tokens. Modify your text generation by including these parameters."
      ],
      "metadata": {
        "id": "mfGKC1D5ODOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = lora_model.generate(**model_inputs,\n",
        "                             max_new_tokens=200, # modify\n",
        "                             top_k=0, # modify\n",
        "                             top_p=0.0, # modify\n",
        "                             temperature=0.0, # modify\n",
        "                             num_beams=0, # modify\n",
        "                             no_repeat_ngram_size = 0, # modify\n",
        "                             do_sample=True,\n",
        "                             pad_token_id=tokenizer.eos_token_id)[0]\n",
        "tuned_story = tokenizer.decode(output)\n",
        "print(tuned_story)"
      ],
      "metadata": {
        "id": "Z7uVtreQODzn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}