# Stat 359 Winter 2024
Stat 359 course materials



# Course Lectures 

Lecture notes can be found on the course Canvas website. 


| Lecture                  |  Date | Material | Readings                
|--------------------------|-------|----------|----------------------------|
| Week 1, Thursday         | January 4 |   Introduction  | [Perplexity](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/), [Perplexity 2](https://web.stanford.edu/~jurafsky/slp3/3.pdf), [Linear Models](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf)  |
| Week 2, Tuesday           | January 9  | Attention |  [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf), [Attention Mechanisms](https://lilianweng.github.io/posts/2018-06-24-attention/), [Attention with code](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html), [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) |
| Week 2. Thursday       | January 11 | Transformers |  [Intro to Transformers](https://arxiv.org/pdf/2304.10557.pdf), [Discussion](https://www.columbia.edu/~jsl2239/transformers.html), [Blog post](https://peterbloem.nl/blog/transformers), [Skip connections](https://theaisummer.com/skip-connections/), [Layer normalization](https://www.kaggle.com/code/halflingwizard/how-does-layer-normalization-work), [Byte-Pair Encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt) |
| Week 3, Tuesday            | January 16 | Coding a transformer |  [Code example](https://buomsoo-kim.github.io/attention/2020/04/21/Attention-mechanism-19.md/) |
| Week 3, Thursday         | January 18| BERT, GPT, LLAMA | [Annotated GPT 2](https://jalammar.github.io/illustrated-gpt2/),  [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [Adversarial attacks on GPT-2](https://arxiv.org/abs/2012.07805) [LLAMA Paper](https://scontent-ord5-1.xx.fbcdn.net/v/t39.8562-6/333078981_693988129081760_4712707815225756708_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=it_GnOgZ1hMAX_qDhzS&_nc_ht=scontent-ord5-1.xx&oh=00_AfCZyg0NnnD2SfBipL7DBQ467rntvBHugEZo7maieJZNTQ&oe=65ACEFE2), [BERT](https://arxiv.org/pdf/1810.04805.pdf), [Understanding BERT](https://jalammar.github.io/illustrated-bert/)|
| Week 4, Tuesday            | January 23| Prompt Tuning, chain of thought, hindsight chain of thought, backwards chain of thought, Graph of Thought, Tree of Thought, Training Chain-of-Thought via Latent-Variable Inferenc, prompt engineering | [Language Models are Few Shot Learners](https://arxiv.org/abs/2005.14165), [Zero shot chain of thought](https://arxiv.org/abs/2205.11916), [LLMs are human-level prompt engineers](https://arxiv.org/abs/2211.01910), [Tree of Thought](https://arxiv.org/abs/2305.10601), [Chain of verification](https://arxiv.org/abs/2309.11495), [Promptbreeder](https://arxiv.org/abs/2309.16797), [Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide?tab=readme-ov-file), [Open-AI Prompting Guide](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results)  |
| Week 4, Thursday          | January 25| Fine tuning, tool use, parameter-efficient fine tuning, LORA, Instruction Tuning (SFT), Neftune, quantization | [LORA](https://arxiv.org/abs/2106.09685), [PEFT](https://huggingface.co/blog/peft), [Quantization](https://arxiv.org/abs/2305.14314), [Quantization Blog](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/),  [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), [ToolEMU](https://toolemu.com/), [NEFTune](https://arxiv.org/abs/2310.05914), [Tool Use code](https://python.langchain.com/docs/modules/agents/how_to/intermediate_steps), [Model Calibration](https://arxiv.org/abs/2012.15723) |
| Week 5, Tuesday        | January 30 | Hugging face, fine tuning LLAMA    | [Mistral Fine Tuning](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb), [LLAMA fine tuning](https://github.com/facebookresearch/llama-recipes/blob/main/examples/quickstart.ipynb)|
| Week 5, Thursday          | February 1| No class | |
| Week 6, Tuesday        | February 6|  RAG, when to use RAG vs SFT, lexacagraphical vs semantic search, sentence transformers, Retrieval transformers and long term memory in transformers, RAG Code. | [RAG](https://arxiv.org/pdf/2312.10997.pdf), [RAG code](https://python.langchain.com/docs/use_cases/question_answering/#quickstart), [Sentence Transformers](https://arxiv.org/abs/1908.10084), [Retrieval Transformers](https://jalammar.github.io/illustrated-retrieval-transformer/), [Improving Neural Language Models with a Continuous Cache](https://openreview.net/forum?id=B184E5qee) |
| Week 6, Thursday       | February 8| ChatGPT and RLHF, rejection sampling, DPO, Gopher Cite   | [Chain of hindsight](https://arxiv.org/abs/2302.02676) |
| Week 7, Tuesday           | February 13| Asking questions about images. Conditional layer norm, FILM, CLIP, BLIP, LAVA |   |
| Week 7, Thursday       | February 15| Diffusion models, DDPM, classifier-free guidance   | |
| Week 8, Tuesday       | February 20| Stable Diffusion, tuning stable diffusion, Brianâ€™s code  |   |
| Week 8, Thursday   | February 22| Frontiers, using LLMs to help diffusion models by planning out images. Instance recognition and inserting new objects %s tricks. Consistency models, SD Edit,  Diffusion in robotics.                                      | |
| Week 9, Tuesday |  February 27| Presentations  | |
| Week 9, Thursday   |  February 29| Presentations |  |
| Week 10, Tuesday   |  March 5th| Presentations  |  |



# Homeworks and Due Dates


| Project title                  | Date released | Due date                
|--------------------------------|---------------|-------------------------|
|   Assignment 1       | Jan 9   | Jan 25  |
|     Assignment 2      |  Jan 30   | Feb 15  |
| Final presentation topic proposals |       |  Feb 15   | 
|  Final presentations        |       | Feb 27  |
