{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stat-junda/Stat-359-Modern-Deep-Learning/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answering Fermi Questions\n",
        "\n",
        "This assignment consists of three parts.\n",
        "\n",
        "1. Basic usage of OpenAI's API and how to construct query calls\n",
        "2. Writing good prompts to get better output from ChatGPT\n",
        "3. Basics of the attention mechanism"
      ],
      "metadata": {
        "id": "5HxRzTn7wUVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the ! pip line of this cell if you get errors running cell 3.\n",
        "# You might get errors when running this. If you are able to run cell 3, then\n",
        "# you can ignore these errors.\n",
        "! pip install cohere tiktoken\n",
        "! pip install openai\n",
        "\n"
      ],
      "metadata": {
        "id": "bWAWt_F4xaes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457f0c7a-3acc-4add-ff48-24c060cd2e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.44)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.3)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.11.17)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1) OpenAI API\n",
        "\n",
        "Read through the documentation of OpenAI API. Start [here](https://github.com/openai/openai-python).\n",
        "\n",
        "\n",
        "You should have an API key. Place your API key in the left tool bar on Colab, by clicking on the tool item the shape of a key, labelled \"secrets\".\n",
        "\n",
        "Add a new secret.\n",
        "\n",
        "Give it the name \"**openai**\" and paste your key in the value section. Make sure to toggle Notebook access.\n",
        "\n"
      ],
      "metadata": {
        "id": "lfiRaFYLurON"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTDtUtR1wCsm"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('openai')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI, ChatCompletion\n",
        "\n",
        "# Instantiate your OpenAI API client\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=API_KEY,\n",
        ")"
      ],
      "metadata": {
        "id": "BB0dHtQbwTb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The key has been set. We are ready to query ChatGPT.\n",
        "# Fill out the functions below.\n",
        "# Do not batch queries.\n",
        "# Do not change the model from 'gpt-3.5-turbo'\n",
        "\n",
        "def basic_query(client: OpenAI, message: str, model: str = 'gpt-3.5-turbo'):\n",
        "  '''\n",
        "  Inputs:\n",
        "  `client`: Takes an OpenAI client object\n",
        "  `message`: The message to pass to the OpenAI LLM model\n",
        "  `model`: The model to use. Do not modify this\n",
        "\n",
        "  Returns:\n",
        "  An OpenAI response object to the message you have passed.\n",
        "  '''\n",
        "  messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ] # fill out the message payload\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    ) # placeholder\n",
        "\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "LEi50yIWt3mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_response(responseObj: ChatCompletion) -> Dict[str, str]:\n",
        "    '''\n",
        "    Inputs:\n",
        "    `responseObj`: The response of the GPT chat.completions.create()\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of the following format:\n",
        "    {\n",
        "      'role': STRING representing the role attribute returned by the response obj\n",
        "      'content': STRING representation of GPT's response message,\n",
        "    }\n",
        "\n",
        "    Hint, run the basic_query() in another code block and observe the output structure.\n",
        "    That alone should be sufficient to make this function.\n",
        "    '''\n",
        "    # Extracting the last message from the response, which is the model's reply\n",
        "    last_message = responseObj.choices[0].message\n",
        "\n",
        "    # Constructing the output dictionary\n",
        "    out = {\n",
        "        'content': last_message.content\n",
        "    }\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "0vYTXlvXtA6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN BUT DO NOT MODIFY THE CODE BELOW\n",
        "\n",
        "message = '''What is a Fermi Question.\n",
        "Explain it concisely and provide a brief example.'''\n",
        "\n",
        "responseObj = basic_query(client=client,\n",
        "                          message= message)\n",
        "\n",
        "parse_response(responseObj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgRTH0cysKVK",
        "outputId": "2eabf9c0-2330-40fb-a370-5d7cd8508a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': 'A Fermi question is a problem-solving technique that involves making rough estimations and using logical reasoning to arrive at an approximate answer. It is named after the physicist Enrico Fermi, who was known for his ability to make quick and accurate estimations.\\n\\nAn example of a Fermi question would be \"How many piano tuners are there in the city of New York?\" To tackle this question, you would break it down into smaller, more manageable components. For instance, you might estimate how many households in New York own pianos, how often they need tuning, and how many pianos a tuner can tune in a day. Then, by multiplying these approximations together, you can arrive at a reasonable estimate for the total number of piano tuners in New York.'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Part 2) Fermi Questions"
      ],
      "metadata": {
        "id": "1YC5y0io4sX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = '''\n",
        "How many tennis balls can fit on a Boeing 747?\n",
        "'''\n",
        "\n",
        "fermiResponseObj = basic_query(client, message=question)\n",
        "parse_response(fermiResponseObj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yDUFDiT5ETY",
        "outputId": "6822e1a2-42b7-48e2-8ee4-93f090fe3af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': 'The exact number of tennis balls that can fit on a Boeing 747 would depend on factors such as the model of the plane, the available space, and how the balls are packed. Without specific measurements and configurations, it is difficult to give an accurate answer. However, it is safe to say that a Boeing 747 has a large cargo hold and can accommodate thousands of tennis balls.'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I ran the code, I got the following result:\n",
        "\n",
        "```\n",
        "The exact number of tennis balls that can fit on a Boeing 747 would depend on various factors, such as the size of the tennis balls, how they are packed, and whether any other cargo is being carried in the plane.\n",
        "Furthermore, there are different models of the Boeing 747 with varying cargo capacities.\n",
        "However, as a rough estimate, to visualize the capacity, let's assume we are using standard-sized tennis balls (approximately 6.7 cm in diameter)\n",
        "and considering only the cargo hold of a Boeing 747-400.\n",
        "\n",
        "The cargo hold of a Boeing 747-400 has a total volume of around 570 cubic meters. Assuming that each tennis ball has a volume of approximately 26.8 cubic centimeters, we can calculate an approximate number.\n",
        "\n",
        "570 cubic meters is equivalent to 570,000,000 cubic centimeters.\n",
        "\n",
        "Thus, the estimated number of tennis balls that can fit would be:\n",
        "\n",
        "570,000,000 cubic centimeters / 26.8 cubic centimeters â‰ˆ 21,268,656 tennis balls.\n",
        "\n",
        "Please note that this is a simplified calculation and does not take into account any additional factors that may affect the total capacity of the aircraft or its weight limitations.\n",
        "```\n",
        "\n",
        "This answer is not good for a couple of reasons\n",
        "  - This seems like quite a relatively specific number rather than a range.\n",
        "  - The more acceptable approach is to compare volumes. This compares weights. This is actually telling us how many tennis balls will it take to weight the same as a 747.\n",
        "\n",
        "Your job is to make these estimates better with prompting. You should aim for a response that provides a reasonable range.\n",
        "\n",
        "E.g. How many tennis balls fit in a Boeing 747?\n",
        "\n",
        "Possible types of responses:\n",
        "\n",
        " - ~Between 100 and 200~. While true and obvious, this is not a very intelligent answer\n",
        " - ~60,000,000 tennis balls~. This answer provides no chain of thought.\n",
        " - Provided the diameter of a tennis ball is x, and the volume of the 747 is z, we estimate around 10^4 to 10^6 tennis balls can fit on a 747. (If your GPT gives this type of response, it would be considered an excellent response.)\n",
        "\n",
        "\n",
        "Your job is to come up with one single prompt, that will answer all fifty Fermi questions below. In reality, you will probably encounter some questions that it cannot solve no matter how you prompt it. That is OK. Try to create a prompt for GPT to yield as many reasonable results as you can. Be creative with your  prompt.\n",
        "\n",
        "\n",
        "**When testing out different prompts, only run it on 1 or 2 questions. Once you have decided on a prompt, go ahead and run it on the entire list.**\n"
      ],
      "metadata": {
        "id": "LjnPHtSr13Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = '''\n",
        "When answering Fermi questions, such as estimating quantities or numbers in various scenarios, it's important to focus on logical reasoning and approximation.\n",
        "Use known averages or typical values for measurements that are not specified.\n",
        "Explain your reasoning in steps, highlighting any assumptions you make.\n",
        "Always aim to provide answers in the form of a reasonable range to reflect the uncertainties involved in such estimations.\n",
        "Your answer should give a general idea rather than a precise figure, demonstrating the thought process behind arriving at that estimate.\n",
        "'''"
      ],
      "metadata": {
        "id": "mazud6Vi9rSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THESE QUESTIONS\n",
        "questions = [\n",
        "  'How many cups of coffee are consumed globally in a day?',\n",
        "  'Estimate the number of bricks used in the construction of the Great Wall of China.',\n",
        "  'How many grains of sand are on an average sized beach?',\n",
        "  'Estimate the total weight of all elephants in Africa in kilograms.',\n",
        "  'How many breaths does an average person take in a year?',\n",
        "  'Estimate the number of hours the average person spends watching TV in a month.',\n",
        "  'How many blades of grass are on a soccer field?',\n",
        "  'Estimate the number of cells in the human brain.',\n",
        "  'How many total hours of sleep does a person get in a year?',\n",
        "  'Estimate the number of text messages sent globally in a minute.',\n",
        "  'How many McDonalds are there in the United States?',\n",
        "  'Estimate the total weight of all the plastic produced in a year.',\n",
        "  'How many footsteps does an average person take in a day?',\n",
        "  'Estimate the daily number of people who visit a large shopping mall.',\n",
        "  'How many gigabytes of data are transferred on the internet in an hour?',\n",
        "  'Estimate the number of busses in the United States.',\n",
        "  'How many trumpet players are there in the world?',\n",
        "  'Estimate the total length of all the rivers in Asia in kilometers.',\n",
        "  'How many calories does the average person consume on Thanksgiving Day?',\n",
        "  'Estimate the number of bacteria on an average human hand.',\n",
        "  'How many books are in all libraries in the United States',\n",
        "  'Estimate the number of atoms in a grain of sand.',\n",
        "  'How many cups of tea are consumed daily around the world?',\n",
        "  'Estimate the total weight of all the paper produced in a year.',\n",
        "  'How many blades of wheat are in a small loaf of bread?',\n",
        "  'Estimate the number of hairs on an average cat.',\n",
        "  'How many breaths does a newborn baby take in an hour?',\n",
        "  'Estimate the number of footsteps taken during a marathon.',\n",
        "  'How many ants are there in a large ant colony?',\n",
        "  'Estimate the number of stars visible in the night sky from a city.',\n",
        "  'How many tweets are sent worldwide in a minute?',\n",
        "  'Estimate the total weight of all the apples sold in a month.',\n",
        "  'How many atoms are there in a human cell?',\n",
        "  'Estimate the number of raindrops in a light rain shower.',\n",
        "  'How many total minutes of music are streamed on Spotify in a day?',\n",
        "  'Estimate the number of computer mice in use globally.',\n",
        "  'How many light bulbs are in Times Square during New Years Eve?',\n",
        "  'Estimate the total length of all the fiber optic cables under the ocean.',\n",
        "  'How many wheels are there in the world?',\n",
        "  'Estimate the number of grains of rice in a 5 kilogram bag.',\n",
        "  'How many blades of grass are in an 18 hole golf course?',\n",
        "  'Estimate the number of words in an average length novel.',\n",
        "  'How many breaths does a person take during a one hour yoga session?',\n",
        "  'Estimate the total weight of all the plastic bottles used in a year.',\n",
        "  'How many footsteps does the average person take in a lifetime?',\n",
        "  'Estimate the number of atoms in a cup of water.',\n",
        "  'How many grains of salt are there in a typical salt shaker?',\n",
        "  'Estimate the total length of all the subway tracks in New York City.',\n",
        "  'How many iPhones are there in the world?',\n",
        "  'Estimate the number of words spoken in the United States in one day.'\n",
        "]"
      ],
      "metadata": {
        "id": "yBy1gA3W96Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_all(PROMPT: str, questions: list) -> list:\n",
        "  '''\n",
        "  Inputs:\n",
        "  `PROMPT`: your prompt as a string\n",
        "  `questions`: the list of questions from above\n",
        "\n",
        "  Returns:\n",
        "  A list of dictionary responses containing 'role' and 'content' keys for each question-response.\n",
        "\n",
        "\n",
        "  Write a function to take in your prompt, append it to all the questions\n",
        "  to send to the API with basic query. Do not use batching.\n",
        "\n",
        "  Return your responses as dictionaries like above\n",
        "  into a list called `answers`.\n",
        "\n",
        "  You may use the functions you have written above:\n",
        "  basic_query() and parse_response()\n",
        "\n",
        "  '''\n",
        "\n",
        "  ## WRITE YOUR FUNCTION HERE\n",
        "\n",
        "  answers = [] # this is a placeholder\n",
        "\n",
        "  # for testing purposes, you can evaluate your prompts on a single question.\n",
        "  # once you are happy with your prompt, you can remove the [:1] below, to\n",
        "  # loop over the entire question list.\n",
        "  for question in questions:\n",
        "        # Combine the prompt with the question\n",
        "        combined_query = PROMPT + '\\n\\n' + question\n",
        "\n",
        "        # Send the combined query to the basic_query function\n",
        "        responseObj = basic_query(client, message=combined_query)\n",
        "\n",
        "        # Parse the response and add it to the answers list\n",
        "        parsed_response = parse_response(responseObj)\n",
        "        answers.append(parsed_response)\n",
        "\n",
        "\n",
        "  return answers\n"
      ],
      "metadata": {
        "id": "F7uQqQTqECAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN BUT DO NOT MODIFY THIS CODE BLOCK.\n",
        "# This code block may take a while to run\n",
        "\n",
        "answer_all(PROMPT=PROMPT, questions=questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSmFhv2ZKeKp",
        "outputId": "b48844b9-c64b-4866-98e6-e57b2539f3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: How many cups of coffee are consumed globally in a day?\n",
            "Answer 1: To estimate the number of cups of coffee consumed globally in a day, we can start by making a few assumptions and approximations:\n",
            "\n",
            "1. Global population: According to the United Nations, the world population is currently around 7.9 billion people.\n",
            "\n",
            "2. Coffee consumption per person: On average, we'll assume that approximately 50% of the global population consumes coffee regularly.\n",
            "\n",
            "3. Cups of coffee per person: Assuming an average coffee intake of 2 cups per day (although this can vary widely), we'll use this as a rough estimate.\n",
            "\n",
            "Based on these assumptions, we can calculate the total cups of coffee consumed globally in a day:\n",
            "\n",
            "Global population consuming coffee = Global population * Coffee consumers percentage\n",
            "= 7.9 billion * 0.5\n",
            "= 3.95 billion people\n",
            "\n",
            "Cups of coffee consumed globally in a day = Global population consuming coffee * Cups of coffee per person\n",
            "= 3.95 billion * 2\n",
            "= 7.9 billion cups\n",
            "\n",
            "Therefore, a reasonable estimate is that globally approximately 7.9 billion cups of coffee are consumed each day. However, it's important to note that this estimate may vary significantly due to regional and cultural differences in coffee consumption habits.\n",
            "\n",
            "Question 2: Estimate the number of bricks used in the construction of the Great Wall of China.\n",
            "Answer 2: To estimate the number of bricks used in the construction of the Great Wall of China, we can follow a logical and approximate approach. It's important to note that the precise number of bricks used is unknown, as there are many sections and variations in the wall. However, we can still make an estimation based on certain assumptions.\n",
            "\n",
            "1. Estimate the length and height of the wall: The Great Wall of China is approximately 21,196 kilometers (13,171 miles) long. The height of the wall varies, but on average, it is around 6 to 8 meters (20 to 26 feet) tall.\n",
            "\n",
            "2. Determine the thickness and width of the wall: The Great Wall typically has an average thickness of about 7 to 8 meters (23 to 26 feet) and a width of about 5 to 6 meters (16 to 20 feet) at the top.\n",
            "\n",
            "3. Calculate the volume of a brick: Assuming the bricks used in the construction of the Great Wall were a standard size, we can estimate their dimensions to be around 30 cm (0.3 meters) long, 15 cm (0.15 meters) wide, and 10 cm (0.1 meters) thick. Therefore, the volume of each brick would be approximately 0.0045 cubic meters (0.3 x 0.15 x 0.1).\n",
            "\n",
            "4. Estimate the number of bricks per square meter: To estimate the number of bricks used per square meter, we need to consider the average thickness and width of the wall. Let's assume that the bricks were laid in a single layer and that each layer of bricks covers an area of approximately 0.072 square meters (0.008 x 0.009) based on the average brick size.\n",
            "\n",
            "5. Calculate the total area of the Great Wall: To estimate the number of bricks used in the entire Great Wall, we need to calculate the total surface area. Assuming a constant height and width throughout the wall, we can approximate the total area using the average height and width values. Therefore, the total area of the wall would be estimated at 21,196 km (21,196,000 meters) x 7.5 meters (average height and width) = 158,970,000 square meters.\n",
            "\n",
            "6. Estimate the total number of bricks: Finally, we can estimate the total number of bricks used in the construction of the Great Wall by multiplying the total area by the number of bricks per square meter. Thus, the estimated number of bricks would be roughly 158,970,000 square meters x (1 layer / 0.072 sqm) = 2,207,500,000 bricks.\n",
            "\n",
            "As a result, based on these approximations and assumptions, we estimate that around 2,207,500,000 bricks were used in the construction of the Great Wall of China. However, this is just an estimation, and the actual number may vary.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3) Implementing Basic Attention\n",
        "\n",
        "Finish this implementation of the attention mechanism. Your result should be an attention matrix with 4 rows and 3 columns.\n",
        "\n",
        "Your job is to implement the `get_attention()` function"
      ],
      "metadata": {
        "id": "7I7GYbNvHGI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import dot\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "class BasicAttention:\n",
        "  def __init__(self):\n",
        "    # Assume these word embeddings\n",
        "    self.word_1 = np.array([1, 0, 0])\n",
        "    self.word_2 = np.array([0, 1, 0])\n",
        "    self.word_3 = np.array([1, 1, 0])\n",
        "    self.word_4 = np.array([0, 0, 1])\n",
        "\n",
        "    # Place the words into an array\n",
        "    self.words = np.array([self.word_1, self.word_2, self.word_3, self.word_4])\n",
        "\n",
        "    # Simulate the weight matrices\n",
        "    # In an actual model, these matrices are learned via optimization\n",
        "    rng = random.RandomState(seed=42)\n",
        "    self.W_Q = rng.randint(3, size=(3, 3))\n",
        "    self.W_K = rng.randint(3, size=(3, 3))\n",
        "    self.W_V = rng.randint(3, size=(3, 3))\n",
        "\n",
        "  def get_attention(self) -> np.ndarray:\n",
        "    # Step 1: Calculate Q, K, V matrices\n",
        "    Q = dot(self.words, self.W_Q)\n",
        "    K = dot(self.words, self.W_K)\n",
        "    V = dot(self.words, self.W_V)\n",
        "\n",
        "    # Step 2: Compute the dot product of Q and the transpose of K,\n",
        "    # then apply softmax to get attention scores\n",
        "    dk = K.shape[1]\n",
        "    scaled_attention_scores = softmax(dot(Q, K.T) / np.sqrt(dk), axis=1)\n",
        "\n",
        "    # Step 3: Multiply the attention scores with V to get the final attention output\n",
        "    attn = dot(scaled_attention_scores, V)\n",
        "\n",
        "    return attn\n"
      ],
      "metadata": {
        "id": "_1D0VNyBHKoB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn = BasicAttention()\n",
        "print(attn.get_attention())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi2rcmpxHnUx",
        "outputId": "de7d9e31-ca4b-4555-d181-8b58cff50045"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.98522025 1.74174051 0.75652026]\n",
            " [0.90965265 1.40965265 0.5       ]\n",
            " [0.99851226 1.75849334 0.75998108]\n",
            " [0.99560386 1.90407309 0.90846923]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NOTE\n",
        "\n",
        "### Due to the generative nature of ChatGPT, your answers can be stochastic when run at different times and may differ depending on your inputs. It is important that you run the code for you to see your response.\n",
        "\n",
        "### Before submitting this assignment, RUN and KEEP the output results on this notebook. If you do not produce or keep these outputs, then the code will be run when it is being graded. Then you will be graded on the outputs from that run.\n",
        "\n",
        "### Best to run your code beforehand, so you know what you are submitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "a_DOk7bE6hsg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SoQfqnerK-_Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}